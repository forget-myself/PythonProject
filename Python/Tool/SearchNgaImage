from bs4 import BeautifulSoup
import requests
import time
import re
import urllib.request
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',
    'Connection':'keep-alive'
}

cookies='UM_distinctid=16eedd75b58642-0e4c9dd250057a-32365f08-1fa400-16eedd75b5979e; taihe_bi_sdk_uid=e211ee74f57916ab6f578997e4017c64; taihe_bi_sdk_session=279a24ba029d736223022c14c4f0b304; ngacn0comUserInfo=%25B7%25E7Nuts%09%25E9%25A3%258ENuts%0939%0939%09%0911%0943602%094%090%090%0919_15%2C61_19%2C67_45%2C87_15; ngaPassportUid=34982491; ngaPassportUrlencodedUname=%25B7%25E7Nuts; ngaPassportCid=Z8f21rkjck7d4fit5ndjjltdmgl2qab6opo7ddgi; lastvisit=1586586529; lastpath=/thread.php?fid=-7; ngacn0comUserInfoCheck=829aee64008646527deca73717bed315; ngacn0comInfoCheckTime=1586586529; bbsmisccookies=%7B%22pv_count_for_insad%22%3A%7B0%3A-43%2C1%3A1586624426%7D%2C%22insad_views%22%3A%7B0%3A1%2C1%3A1586624426%7D%2C%22uisetting%22%3A%7B0%3A%22a%22%2C1%3A1586586818%7D%7D; CNZZDATA30043604=cnzz_eid%3D1863004508-1575948152-%26ntime%3D1586581294; _cnzz_CV30043604=forum%7Cfid-7%7C0'
cookie={}
for line in cookies.split(';'):
    name,value=line.strip().split('=',1)
    cookie[name]=value

urls=['https://bbs.nga.cn/thread.php?fid=-7&page=',
      'https://bbs.nga.cn/thread.php?fid=436&page='
      ]


#获取内容里的img标签
def parseContent(content):
    pattern = "\[img](.*?)\[/img]"
    result = re.findall(pattern, content)
    return result

#获取指定页数的主题
def getTopic(url,number=1):
    topics = []
    for i in range(1,number+1):
        url_one=url+str(i)
        res=requests.get(url_one,cookies=cookie,headers=headers)
        soup=BeautifulSoup(res.content,'lxml')
        topic=soup.find_all('a',class_='topic')
        for t in topic:
            topics.append('https://bbs.nga.cn/'+t['href']+"  "+t.get_text())
    return topics


#def saveImage(url):


imageNum=0
def showUrl(oneUrl) :
    if oneUrl.startswith('http') == False:
        oneUrl = 'https://img.nga.178.com/attachments'+oneUrl.strip( '.' )
    #print(oneUrl[-15:])
    #print(oneUrl[-15:].replace(r'/',''))
    global imageNum
    a = oneUrl[-15:].replace(r'/','')
    print(a)
    urllib.request.urlretrieve(oneUrl, 'I:/pythonImage/Image/ops_'+str(imageNum) + a)
    imageNum+=1
    print(imageNum)

#获取指定主题的内容
def getContext(url):
    #print("url: ",url)
    res=requests.get(url,cookies=cookie,headers=headers)
    #print("content: ",res.content)
    soup=BeautifulSoup(res.content,'lxml')
    title_body = soup.find_all('p', class_='postcontent ubbcode')
    count =0
    for t in title_body:
        if count<3:
            print(t.strings)
        elif count == len(title_body)-1 :
            print(t.strings)
        count+=1
    context = soup.find_all('span', class_="postcontent ubbcode")
    for c in context:
        urls = parseContent(c.get_text())
        if len(urls)>0:
            for k in range(len(urls )):
                showUrl(urls[k])
    global totalPageNum
    if totalPageNum == 1:
        res.encoding = res.apparent_encoding
        soup2 = BeautifulSoup(res.content, 'html.parser')
        table_body = soup2.find_all(id='m_pbtnbtm')
        for th in table_body:
            th_all22 = th.find_all('script')
            # print(th_all22)
            if len(th_all22) == 2:
                totalPageNum = int(re.split(r'[:,]', str(th_all22[1]))[3])  # 多字符分隔（;或者,）
                print('有几页 ', totalPageNum)

def getTheme(index,big_topic,page_num):
    if (index == -1):
        Topic = getTopic(urls[big_topic - 1], page_num)
        for i in range(1, len(Topic)):
            print(str(i) + " " + Topic[i - 1])
        index = int(input(""))
    #getTheme(   index,big_topic,page_num)

page_num=0
def getPageNum(page):
    page+=1
    if page>2 :
        page=1
    return page

currPageNum=1
totalPageNum=1

if __name__ == '__main__':
   big_topic=1
   page_num = getPageNum(page_num)
   Topic=getTopic(urls[big_topic-1],page_num)
   #print(len(Topic),page_num)
   opsnum = 0
   '''
   #打印所有的帖子标题
   for i in range(1,len(Topic)):
       print(str(i)+" "+Topic[i-1])
       '''
   for i in range(1000):
       for j in range(1,len(Topic)):
           top_id=j
           getTheme(top_id,big_topic,page_num)
           url_next=Topic[top_id-1].split(" ")[0]
           # 打印链接和标题
           print("url_next",url_next)
           while currPageNum <= totalPageNum:
               if currPageNum>1:
                   url_next += r'&page='+ str(currPageNum)
                   print("url_next2", url_next)
               getContext(url_next)
               currPageNum += 1
               time.sleep(2)
           currPageNum=1
           totalPageNum=1





